{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Laws\n",
    "\n",
    "In this week we want to create simple scaling laws for language models. We will follow approach two in Hoffmann et al. 2022 https://arxiv.org/abs/2203.15556. \n",
    "\n",
    "- We train GPT models based on nanoGPT: https://github.com/karpathy/nanoGPT\n",
    "\n",
    "- Download the dataset gutenberg_poetry.txt from MOODLE. This text is a filtered version of https://huggingface.co/datasets/biglam/gutenberg-poetry-corpus.\n",
    "\n",
    "- You can do the training in https://colab.google/.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Scaling laws will be based on the following models. \n",
    "gpt_models = {\n",
    "    'gpt-0':    dict(n_layer=2, n_head=2, n_embd=96),     \n",
    "    'gpt-1':    dict(n_layer=4, n_head=4, n_embd=96),    \n",
    "    'gpt-2':    dict(n_layer=9, n_head=8, n_embd=96),     \n",
    "    'gpt-3':    dict(n_layer=10, n_head=8, n_embd=128),   \n",
    "    'gpt-4':    dict(n_layer=20, n_head=16, n_embd=128),  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Encode the gutenberg_poetry.txt file using prepare.py from nanoGPT/shakespeare_char. Create the models from gpt_models (see above) and calculate the number of non-embedding paramters $N$. (Hint: see model.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: For the models from above we vary the number of training tokens such that we have constant amount of compute $C$. The compute budget can be approximated by $C \\approx 6ND$ where $N$ is the model size and $D$ the number of processed training tokens. We fix three compute values $C_0=6\\cdot 10^{13}$, $C_1=3\\cdot 10^{14}$, and $C_2=6\\cdot 10^{14}$. For $C_0$, we train the models gpt-0, gpt-1, gpt_2, and gpt-3. For $C_1$, we use gpt-1, gpt-2, and gpt-3. For $C_2$, use the models gpt-1, gpt-2, gpt-3, and gpt-4. Derive a formula for the number of training steps $S$ and create a table with the corresponding training steps for each training run. We assume fixed context length of $32$ and batch size of $64$. We train on one GPU with gradient accumulation steps of 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following values for training. \n",
    "eval_interval = 250 \n",
    "eval_iters = 200\n",
    "log_interval = 10 \n",
    "\n",
    "always_save_checkpoint = False\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "batch_size = 64\n",
    "block_size = 32 \n",
    "\n",
    "learning_rate = 1e-3 \n",
    "max_iters = None # needs to be calculated -> use the value S calcualted above \n",
    "lr_decay_iters = None # make equal to max_iters usually\n",
    "min_lr = 1e-4 \n",
    "beta2 = 0.99\n",
    "\n",
    "warmup_iters = 100 \n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Write down the final training loss values $L$ for each run. Plot $L$ as a function of $N$ and fit a curve for each compute value, see Figure 3 (left) Hoffmann et al. 2022. Extract the coordiantes of the minimum for each fitted curve. With the minimum values you can produce Figure 3 (center) and Figure 3 (right) of Hoffmann et al. 2022. Note, $N_{opt}\\; = N_0 \\; C^a$ and $D_{opt}\\; = D_0\\; C^b$. Write down the values for your your scaling laws and create a plot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
